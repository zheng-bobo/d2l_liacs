{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd9a346e-6dfd-4381-a892-25c1046854df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Implement the XOR network and the Gradient Descent Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb45492-aa73-44cd-ab73-c356db18565b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "\n",
    "# Load XOR data\n",
    "def load_xor() -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    X = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
    "    y = torch.tensor([0, 1, 1, 0], dtype=torch.float32)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd6e1ab-7d1b-4bf3-b166-5566d8b517e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dsigmoid(h):\n",
    "    # h is already the output of sigmoid(z)\n",
    "    return h * (1 - h)\n",
    "\n",
    "\n",
    "def dtanh(h):\n",
    "    # h is already the output of tanh(z)\n",
    "    return 1 - h**2\n",
    "\n",
    "\n",
    "def drelu(z):\n",
    "    return (z > 0).float()\n",
    "\n",
    "\n",
    "class XORNet:\n",
    "    \"\"\"\n",
    "    Minimal 2-2-1 MLP for learning XOR.\n",
    "    Total parameters = 9: W1(2x2)=4, b1(2)=2, W2(2)=2, b2(1)=1\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        activation=\"sigmoid\",\n",
    "        weight_scale: float = 0.5,\n",
    "        seed: int = 42,\n",
    "        dtype=torch.float32,\n",
    "    ):\n",
    "        self.dtype = dtype\n",
    "        self.seed = seed\n",
    "\n",
    "        if self.seed is not None:\n",
    "            torch.manual_seed(self.seed)\n",
    "\n",
    "        self.W1 = torch.normal(\n",
    "            mean=0.0, std=weight_scale, size=(2, 2), dtype=self.dtype\n",
    "        )\n",
    "        self.b1 = torch.normal(mean=0.0, std=weight_scale, size=(2,), dtype=self.dtype)\n",
    "        self.W2 = torch.normal(mean=0.0, std=weight_scale, size=(2,), dtype=self.dtype)\n",
    "        self.b2 = torch.normal(mean=0.0, std=weight_scale, size=(), dtype=self.dtype)\n",
    "\n",
    "        if activation == \"sigmoid\":\n",
    "            self.act = torch.sigmoid\n",
    "        elif activation == \"tanh\":\n",
    "            self.act = torch.tanh\n",
    "        elif activation == \"relu\":\n",
    "            self.act = torch.relu\n",
    "        else:\n",
    "            raise ValueError(\"Unknown activation function\")\n",
    "\n",
    "        self.activation_name = activation\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        X: (N,2)\n",
    "        Returns y_hat and cache (for backpropagation)\n",
    "        \"\"\"\n",
    "        z1 = X @ self.W1 + self.b1  # (N,2)\n",
    "        h = self.act(z1)  # (N,2)\n",
    "        z2 = h @ self.W2 + self.b2  # (N,)\n",
    "        y_hat = torch.sigmoid(z2)  # Output layer uses sigmoid\n",
    "        cache = {\"X\": X, \"z1\": z1, \"h\": h, \"z2\": z2, \"y_hat\": y_hat}\n",
    "        return y_hat, cache\n",
    "\n",
    "    @staticmethod\n",
    "    def mse(y_hat: torch.Tensor, y: torch.Tensor) -> float:\n",
    "        return float(torch.mean((y_hat - y) ** 2).item())\n",
    "\n",
    "    @staticmethod\n",
    "    def miscls(y_hat: torch.Tensor, y: torch.Tensor, thr: float = 0.5) -> int:\n",
    "        y_bin = torch.zeros_like(y_hat, dtype=torch.int)\n",
    "        y_bin[y_hat >= thr] = 1\n",
    "        return int(torch.sum(y_bin != y).item())\n",
    "\n",
    "    def gradients(\n",
    "        self, cache: Dict[str, torch.Tensor], y: torch.Tensor\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Compute gradients for the whole batch (4 samples). Loss L = mean((y_hat-y)^2)\n",
    "        Let N=4. Chain rule:\n",
    "          dL/dy_hat = 2/N * (y_hat - y)\n",
    "          y_hat = sigmoid(z2) → dy_hat/dz2 = y_hat*(1-y_hat)\n",
    "          z2 = h·W2 + b2\n",
    "          h = act(z1), z1 = X·W1 + b1\n",
    "        \"\"\"\n",
    "        X = cache[\"X\"]  # (N,2)\n",
    "        z1 = cache[\"z1\"]  # (N,2)\n",
    "        h = cache[\"h\"]  # (N,2)\n",
    "        z2 = cache[\"z2\"]  # (N,)\n",
    "        y_hat = cache[\"y_hat\"]  # (N,)\n",
    "\n",
    "        N = X.shape[0]\n",
    "        dL_dyhat = (2.0 / N) * (y_hat - y)  # (N,)\n",
    "\n",
    "        # Output layer: sigmoid\n",
    "        dyhat_dz2 = y_hat * (1.0 - y_hat)  # (N,)\n",
    "        dL_dz2 = dL_dyhat * dyhat_dz2  # (N,)\n",
    "\n",
    "        # For W2, b2\n",
    "        # z2_i = sum_j h_ij * W2_j + b2\n",
    "        dL_dW2 = h.t() @ dL_dz2  # (2,)\n",
    "        dL_db2 = torch.sum(dL_dz2)  # ()\n",
    "\n",
    "        # Backprop to hidden layer output h\n",
    "        dL_dh = dL_dz2.unsqueeze(1) * self.W2.unsqueeze(0)  # (N,2)\n",
    "\n",
    "        # Hidden layer activation derivative\n",
    "        if self.activation_name == \"sigmoid\":\n",
    "            dh_dz1 = dsigmoid(h)  # (N,2)\n",
    "        elif self.activation_name == \"tanh\":\n",
    "            dh_dz1 = dtanh(h)\n",
    "        elif self.activation_name == \"relu\":\n",
    "            dh_dz1 = drelu(z1)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown activation function\")\n",
    "\n",
    "        dL_dz1 = dL_dh * dh_dz1  # (N,2)\n",
    "\n",
    "        # z1 = X·W1 + b1\n",
    "        dL_dW1 = X.t() @ dL_dz1  # (2,2)\n",
    "        dL_db1 = torch.sum(dL_dz1, dim=0)  # (2,)\n",
    "\n",
    "        return {\n",
    "            \"W1\": dL_dW1,\n",
    "            \"b1\": dL_db1,\n",
    "            \"W2\": dL_dW2,\n",
    "            \"b2\": dL_db2,\n",
    "        }\n",
    "\n",
    "    def step(self, grads: Dict[str, torch.Tensor], lr: float) -> None:\n",
    "        self.W1 -= lr * grads[\"W1\"]\n",
    "        self.b1 -= lr * grads[\"b1\"]\n",
    "        self.W2 -= lr * grads[\"W2\"]\n",
    "        self.b2 -= lr * grads[\"b2\"]\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        X: torch.Tensor,\n",
    "        y: torch.Tensor,\n",
    "        lr: float = 0.5,\n",
    "        epochs: int = 5000,\n",
    "        verbose_every: int = 500,\n",
    "    ) -> Dict[str, list]:\n",
    "        history = {\"mse\": [], \"mis\": []}\n",
    "        for ep in range(1, epochs + 1):\n",
    "            y_hat, cache = self.forward(X)\n",
    "            loss = self.mse(y_hat, y)\n",
    "            mis = self.miscls(y_hat, y)\n",
    "            history[\"mse\"].append(loss)\n",
    "            history[\"mis\"].append(mis)\n",
    "\n",
    "            grads = self.gradients(cache, y)\n",
    "            self.step(grads, lr)\n",
    "\n",
    "            if verbose_every and (ep % verbose_every == 0):\n",
    "                print(f\"[{ep:5d}] mse={loss:.6f} mis={mis}\")\n",
    "        return history\n",
    "\n",
    "    def predict(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        y_hat, _ = self.forward(X)\n",
    "        return (y_hat >= 0.5).int()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml39",
   "language": "python",
   "name": "ml39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
